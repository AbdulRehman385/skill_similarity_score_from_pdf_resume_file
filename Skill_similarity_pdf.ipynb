{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install PyPDF2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RZDQerEpT8fk",
        "outputId": "20871f0f-03d4-4e2a-c456-8808174b02dc"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
            "Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/232.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━\u001b[0m \u001b[32m225.3/232.6 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyPDF2\n",
            "Successfully installed PyPDF2-3.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import PyPDF2\n",
        "\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    with open(pdf_path, 'rb') as file:\n",
        "        reader = PyPDF2.PdfReader(file)\n",
        "        text = ''\n",
        "        for page in reader.pages:\n",
        "            text += page.extract_text()\n",
        "    return text\n",
        "\n",
        "pdf_path = '/content/My_CV.pdf'\n",
        "text = extract_text_from_pdf(pdf_path)\n",
        "print(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sXwjFAL1QfuB",
        "outputId": "9c927fda-5ce7-4ce1-8811-198bcea61bb3"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Flower Classification Using Convolutional Neural Networks (CNN): \n",
            "View: https://www.kaggle.com/code/arehman123/flower-classification-cnn \n",
            "Implemented CNN model for Flower Images Classification using keras. \n",
            "Preprocessed data using image augmentation techniques such as, rotation, \n",
            "flipping, and scaling for robustness. Abdul Rehman \n",
            "Junior Machine Learning Engineer \n",
            "PROFESSIONAL OBJECTIVE \n",
            "CONTACT: Enthusiastic junior machine learning engineer with a background in  \n",
            "economics and hands-on experience with Machine Learning and deep  \n",
            "learning models like Random Forest, XGBOOST, LSTM, GRU, and  CNN.  \n",
            "Skilled in data preprocessing, data visualization, and creating and  \n",
            "training ML models using Python libraries such as TensorFlow and  \n",
            "scikit-learn. Eager to apply my skills and gain real-world experience  \n",
            "through a junior ML/DL role. \n",
            "EDUCATION \n",
            "Bachelor of Science: Economics (2024, July) \n",
            "Sukkur IBA University - Sukkur, Sindh Pakistan \n",
            "● University Level Courses: \n",
            "● C++, R Programming, Econometrics and Statistics \n",
            "● Coursera Certificates: \n",
            "● Supervised Machine Learning: Regression and Classification –  \n",
            "Coursera  (2024) \n",
            "● Advanced Learning Algorithm– Coursera  (2024) \n",
            "● Supervised Machine Learning: Classification - Coursera  (2024) \n",
            "● Introduction to data science in python – Coursera  (2024) \n",
            "● Deep Learning and Reinforcement Learning – Coursera  (2024) \n",
            "Projects: \n",
            "Sarcasm Detection Using RNN, LSTM, and GRU: \n",
            "View: https://www.kaggle.com/code/arehman123/sarcasm-rnn-lstm-gru \n",
            "Built and trained using RNN, LSTM and GRU algorithms. Preprocessed text \n",
            "data using lemmatization, tokenization techniques using spaCy and keras. At \n",
            "last obtained highest ROC score (0.999) for all models. \n",
            "Apple Quality Prediction Using Machine Learning: \n",
            "View: https: //github.com/AbdulRehman385/Apple-Quality-Prediction \n",
            "Developed Decision Tree and Random Forest models using scikit learn tool. \n",
            "For to preprocess the data using Normalization for scaling and for optimizing \n",
            "used RandomSearchCV method. Therefore deployed using streamlit app. Address \n",
            "Islamabad, Pakistan \n",
            "Phone \n",
            "+923163434525 \n",
            "E-mail \n",
            "abdulrehmanshaikh787@gmail.com \n",
            "Linkedin Profile \n",
            "linkedin.com/in/abdul-rehman-shai\n",
            "kh-1a896a2b1 \n",
            "Skills: \n",
            "Programming Languages:  \n",
            "Python, R, C++ \n",
            "Machine Learning and Data  \n",
            "Science tools: TensorFlow, Keras,  \n",
            "scikit-learn \n",
            "Data Visualization tools:  \n",
            "Matplotlib, Seaborn \n",
            "Model Building: Random Forest,  \n",
            "Decision Tree, XGBOOST,  \n",
            "Gradient Boost, SVM, KNN, LSTM,  \n",
            "GRU, CNN, RNN, NLP \n",
            "Data Preprocessing: Pandas,  \n",
            "NumPy, Excel, spaCy \n",
            "Other Tools : Jupyter Notebooks,  \n",
            "PyCharm \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Apply NER\n",
        "doc = nlp(text)\n",
        "skills = []\n",
        "# Print Named Entities\n",
        "for ent in doc.ents:\n",
        "    print(f\"{ent.text}\")\n",
        "    skills.append(ent.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f8d9UYvBTr6-",
        "outputId": "4ccb71ea-279f-4698-b162-67d454e4f4e8"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CNN\n",
            "CNN\n",
            "Flower Images Classification\n",
            "Abdul Rehman \n",
            "Junior\n",
            "Machine Learning\n",
            "Random Forest\n",
            "XGBOOST\n",
            "CNN\n",
            "ML\n",
            "Python\n",
            "TensorFlow\n",
            "ML/DL\n",
            "Bachelor of Science: Economics\n",
            "2024, July\n",
            "Sukkur IBA University - Sukkur\n",
            "Sindh\n",
            "Econometrics\n",
            "2024\n",
            "2024\n",
            "Classification - Coursera\n",
            "2024\n",
            "Coursera\n",
            "2024\n",
            "Coursera\n",
            "2024\n",
            "Sarcasm Detection Using\n",
            "RNN\n",
            "ROC\n",
            "0.999\n",
            "Apple Quality Prediction Using Machine Learning\n",
            "Random Forest\n",
            "Normalization\n",
            "RandomSearchCV\n",
            "Islamabad\n",
            "Pakistan\n",
            "C++ \n",
            "Machine Learning\n",
            "TensorFlow\n",
            "Keras\n",
            "Data Visualization\n",
            "Matplotlib\n",
            "Random Forest\n",
            "XGBOOST\n",
            "Gradient Boost\n",
            "SVM\n",
            "KNN\n",
            "CNN\n",
            "RNN\n",
            "NLP\n",
            "Pandas\n",
            "Excel\n",
            "Jupyter Notebooks\n",
            "PyCharm\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(skills)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "018CvN1tVDEY",
        "outputId": "d186ff66-b3bc-48f9-eb95-704010b1ab0d"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['CNN', 'CNN', 'Flower Images Classification', 'Abdul Rehman \\nJunior', 'Machine Learning', 'Random Forest', 'XGBOOST', 'CNN', 'ML', 'Python', 'TensorFlow', 'ML/DL', 'Bachelor of Science: Economics', '2024, July', 'Sukkur IBA University - Sukkur', 'Sindh', 'Econometrics', '2024', '2024', 'Classification - Coursera', '2024', 'Coursera', '2024', 'Coursera', '2024', 'Sarcasm Detection Using', 'RNN', 'ROC', '0.999', 'Apple Quality Prediction Using Machine Learning', 'Random Forest', 'Normalization', 'RandomSearchCV', 'Islamabad', 'Pakistan', 'C++ \\nMachine Learning', 'TensorFlow', 'Keras', 'Data Visualization', 'Matplotlib', 'Random Forest', 'XGBOOST', 'Gradient Boost', 'SVM', 'KNN', 'CNN', 'RNN', 'NLP', 'Pandas', 'Excel', 'Jupyter Notebooks', 'PyCharm']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "def score_resume_cosine(job_desc_skills, resume_skills):\n",
        "    # Combine job description skills and resume skills into a single list\n",
        "    all_skills = job_desc_skills + resume_skills\n",
        "\n",
        "    # Initialize TfidfVectorizer\n",
        "    vectorizer = TfidfVectorizer()\n",
        "\n",
        "    # Fit and transform the skills into TF-IDF vectors\n",
        "    tfidf_matrix = vectorizer.fit_transform(all_skills)\n",
        "\n",
        "    # Separate job description vector and resume vector\n",
        "    job_desc_vector = tfidf_matrix[:len(job_desc_skills)]  # First part is job description skills\n",
        "    resume_vector = tfidf_matrix[len(job_desc_skills):]     # Second part is resume skills\n",
        "\n",
        "    # Compute cosine similarity between the job description and resume vectors\n",
        "    similarity = cosine_similarity(resume_vector, job_desc_vector)\n",
        "\n",
        "    # Average the similarity scores (or use max, depending on your use case)\n",
        "    average_similarity = similarity.mean()\n",
        "\n",
        "    # Return the score as a percentage\n",
        "    return round(average_similarity * 100, 2)\n",
        "\n",
        "# Example usage\n",
        "job_desc_skills = ['machine learning', 'nlp', 'tensorflow', 'python']\n",
        "resume_skills = ['python', 'nlp', 'machine learning', 'sql']\n",
        "\n",
        "score = score_resume_cosine(job_desc_skills, skills)\n",
        "print(f\"Resume Score (Cosine Similarity): {score}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lpV0orB4XKdM",
        "outputId": "a3e8e893-8177-4130-e62f-7cc7344a070d"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resume Score (Cosine Similarity): 3.12%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def suggested_skills(job_skills, resume_skills):\n",
        "  suggested_skills = []\n",
        "  for skill in job_skills:\n",
        "    if skill not in resume_skills:\n",
        "      suggested_skills.append(skill)\n",
        "  return suggested_skills\n",
        "\n",
        "suggested_skills = suggested_skills(job_desc_skills, [skill.lower() for skill in skills])\n",
        "print(suggested_skills)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Qzvm1QCXazS",
        "outputId": "33bea6db-f0f5-4e58-ce87-3fba1ab64fc6"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['data science']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "job = 'Machine learning, Deep learning, CNN, Java Script, C++'\n",
        "job_skills = job.split(',')\n",
        "print([skill.strip() for skill in job_skills])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1WO9xH1yksN4",
        "outputId": "787662d4-b1bc-487f-8e15-b9db62ad4d77"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Machine learning', 'Deep learning', 'CNN', 'Java Script', 'C++']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "c_K4XxEGoPMJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}